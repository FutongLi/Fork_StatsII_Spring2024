return(p_value)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1
ks_test_normal(data)
ks_test_normal(data)
#
return(paste('p-value ='p_value)
#
return(paste('p-value ='p_value))
#
return(paste('p-value =', p_value))
#
return(D, p_value))
#
return(D, p_value)
#
return(c(D, p_value))
#
return(D)
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, 1000)-1)^2*pi^2)/(8*D^2)))
#
return(D)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test_normal(data)
pkstwo(0.1347281, 1000)
lapply("supp"),  pkgTest)
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
install.packages("supp")
library("supp")
library(supp)
install.packages("supp")
library(supp)
class(data)
str(data)
length(data)
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
#p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, 1000)-1)^2*pi^2)/(8*D^2)))
p_value <- 1-exp(-2*length(data)*D^2)
#
return(D)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test_normal(data)
ks.test(data, "pnorm", mean = 0, sd = 1)
#
return(p_value)
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
#p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, 1000)-1)^2*pi^2)/(8*D^2)))
p_value <- 1-exp(-2*length(data)*D^2)
#
return(p_value)
}
# generate Cauchy random variables
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test_normal(data)
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
#p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, 1000)-1)^2*pi^2)/(8*D^2)))
p_value <- 1 - sum((-1)^seq(1:1000) * exp(-2 * (1:100)^2 * D^2))
#
return(p_value)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test_normal(data)
ks.test(data, "pnorm", mean = 0, sd = 1)
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
D <- max(abs(empiricalCDF - normalCDF))
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
normalCDF <- pnorm(data)
D <- max(abs(empiricalCDF - normalCDF))
p_value <- 1 - sum((-1)^seq(1:1000) * exp(-2 * (1:100)^2 * D^2))
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, length(data))-1)^2*pi^2)/(8*D^2)))
#p_value <- 1 - sum((-1)^seq(1:1000) * exp(-2 * (1:100)^2 * D^2))
#
return(p_value)
}
ks_test_normal(data)
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, length(data))-1)^2*pi^2)/(8*D^2)))
#p_value <- 1 - sum((-1)^seq(1:1000) * exp(-2 * (1:100)^2 * D^2))
#
return(D)
}
ks_test_normal(data)
ks.test(data, "pnorm", mean = 0, sd = 1)
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Define the objective function for OLS regression
ols_objective <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
# Initial values for beta
initial_beta <- c(0, 1)
# Use the BFGS optimization method to estimate the parameters
result_bfgs <- optim(par = initial_beta, fn = ols_objective, x = data$x, y = data$y, method = "BFGS")
# Estimated coefficients
coefficients_bfgs <- result_bfgs$par
print(coefficients_bfgs)
# Equivalent results using lm()
lm_result <- lm(y ~ x, data)
print(coef(lm_result))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
data$y
data
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Define the objective function for OLS regression
ols_objective <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
# Use the BFGS optimization method to estimate the parameters
result_bfgs <- optim(fn = ols_objective, par = 0:1, x = data$x, y = data$y, method = "BFGS")
# Estimated coefficients
print(result_bfgs$par)
# Equivalent results using lm()
lm_result <- lm(y ~ x, data)
print(coef(lm_result))
?optim
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, length(data))-1)^2*pi^2)/(8*D^2)))
#p_value <- 1 - sum((-1)^seq(1:1000) * exp(-2 * (1:100)^2 * D^2))
#return the test statistic and p-value
return(list(D = D, p_value = p_value))
}
# generate Cauchy random variables
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test_normal(data)
ks.test(data, "pnorm", mean = 0, sd = 1)
?ks.test()
ks.test(c(1, 2, 2, 3, 3), c(1, 2, 3, 3, 4, 5, 6), exact = TRUE)
ks.test(c(1, 2, 2, 3, 3),'pnorm')
ks.test(c(1, 2, 2, 3, 3),'pnorm', 0, 1)
ks_test_normal(c(1, 2, 2, 3, 3))
D <- 0.274
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, length(data))-1)^2*pi^2)/(8*D^2)))
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, l0)-1)^2*pi^2)/(8*D^2)))
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, 10)-1)^2*pi^2)/(8*D^2)))
p_value
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
d <- D*sqrt(length(data))
#generate p value
p_value <- (sqrt(2*pi)/d) * sum(exp((-(2*seq(1, length(data))-1)^2*pi^2)/(8*d^2)))
#return the test statistic and p-value
return(list(D = D, p_value = p_value))
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test_normal(data)
ks.test(data, "pnorm", mean = 0, sd = 1)
m_multiply <- function(A, B, m) {
C <- matrix(0, nrow = m, ncol = m)
for (i in 1:m) {
for (j in 1:m) {
s <- 0
for (k in 1:m) {
s <- s + A[i, k] * B[k, j]
}
C[i, j] <- s
}
}
return(C)
}
m_power <- function(A, m, n) {
if (n == 1) {
return(A)
}
V <- m_power(A, m, n/2)
B <- m_multiply(V, V, m)
if (n %% 2 == 0) {
return(B)
} else {
return(m_multiply(A, B, m))
}
}
K <- function(n, d) {
s <- d^2 * n
if (s > 7.24 || (s > 3.76 && n > 99)) {
return(1 - 2 * exp(-(2.000071 + 0.331/sqrt(n) + 1.409/n) * s))
}
k <- as.integer(n * d) + 1
m <- 2 * k - 1
h <- k - n * d
H <- matrix(0, nrow = m, ncol = m)
for (i in 1:m) {
for (j in 1:m) {
if (i - j + 1 < 1) {
H[i, j] <- 0
} else {
H[i, j] <- 1
}
}
}
for (i in 1:m) {
H[i, 1] <- H[i, 1] - h^i
H[m, i] <- H[m, i] - h^(m - i)
}
H[m, 1] <- H[m, 1] + ifelse(2 * h - 1 > 0, 2 * h - 1, 0)
for (i in 1:m) {
for (j in 1:m) {
if (i - j + 1 > 0) {
for (g in 1:(i - j + 1)) {
H[i, j] <- H[i, j] / g
}
}
}
}
eH <- 0
Q <- m_power(H, m, n)
s <- Q[k, k]
for (i in 1:n) {
s <- s * i / n
if (s < 1e-140) {
s <- s * 1e140
eH <- eH - 140
}
}
s <- s * 10^eH
return(s)
}
n <- 100
d <- 0.1
p_value <- K(n, d)
print(p_value)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
d <- D
p_value <- K(n, d)
print(p_value)
ks.test(data, "pnorm", mean = 0, sd = 1)
n <- 100
d <- 0.1
p_value <- K(n, d)
print(p_value)
ks.test(data, "pnorm", mean = 0, sd = 1, alternative = 'gr')
ks.test(data, "pnorm", mean = 0, sd = 1, alternative = 'le')
D <- max(abs(empiricalCDF - normalCDF))
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, 10)-1)^2*pi^2)/(8*D^2)))
ks.test(data, "pnorm", mean = 0, sd = 1, alternative = 'two.sided')
ks.test(data, "pnorm", mean = 0, sd = 1, alternative = 'greater')
p_value *2
D <- max(abs(empiricalCDF - normalCDF))
D <- 2 * D
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, 10)-1)^2*pi^2)/(8*D^2)))
?limit
?limit
install.packages("calculus")
library(calculus)
library(calculus)
?limit
??limit
?limit
install.packages("mosaic")
library(mosaic)
lim(expression, as = value)
?lim
library(pracma)
install.packages("pracma")
library(pracma)
library(pracma)
?lim
#return the test statistic and p-value
return(list(D = D, p_value = p_value))
ks_test_normal <- function(data){
#create empiral distribution of input data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
#create reference distribution CDF
normalCDF <- pnorm(data)
#generate statistic: largerst absolute difference value
D <- max(abs(empiricalCDF - normalCDF))
#generate p value
p_value <- (sqrt(2*pi)/D) * sum(exp((-(2*seq(1, length(data))-1)^2*pi^2)/(8*D^2)))
#return the test statistic and p-value
return(list(D = D, p_value = p_value))
}
#generate Cauchy random variables
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
#execute the test
ks_test_normal(data)
#check the test
ks.test(data, "pnorm", mean = 0, sd = 1)
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
#calculate the RSS for an OLS regression
RSS_ols <- function(beta, x, y) {
y_hat <- beta[1] + beta[2] * x
sum((y - y_hat)^2)
}
#estimate the parameters using BFGS method
bfgs_result <- optim(fn = RSS_ols, par = 0:1, x = data$x, y = data$y, method = "BFGS")
#extract estimated coefficients after optimization
print(bfgs_result$par)
#get the equivalent result using lm()
lm_result <- lm(y ~ x, data)
print(coef(lm_result))
########################
# Tutorial 4 solutions #
########################
## Load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse"),  pkgTest)
## Reading in the data
#  Option 1:
#  Using stringsAsFactors
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt",
stringsAsFactors = TRUE)
#  Option 2:
#  Parse column names as a vector to colClasses
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt",
colClasses = c("hsgrad" = "factor",
"nonwhite" = "factor",
"mhs" = "factor",
"fhs" = "factor",
"intact" = "factor"))
summary(graduation)
# Drop problematic cases
graduation <- graduation[-which(graduation$nsibs < 0),]
#  Option 3:
#  Coerce from a character vector to a logical vector
graduation$hsgrad <- as.logical(as.numeric(as.factor(graduation$hsgrad))-1)
#  Option 4:
#  Use ifelse() with as.logical()...
as.logical(ifelse(graduation$hsgrad == "Yes", 1, 0))
## a) Run the logit regression
mod <- glm(hsgrad ~ ., # period functions as omnibus selector (kitchen sink additive model)
data = graduation,
family = "binomial")
mod <- glm(hsgrad ~ .,
data = graduation,
family = binomial(link = "logit")) # same as above (logit is default arg)
summary(mod)
## Likelihood ratio test
#  Create a null model
nullMod <- glm(hsgrad ~ 1, # 1 = fit an intercept only (i.e. sort of a "mean")
data = graduation,
family = "binomial")
#  Run an anova test on the model compared to the null model
anova(nullMod, mod, test = "Chisq")
anova(nullMod, mod, test = "LRT") # LRT is equivalent
##  Extracting confidence intervals (of the coefficients)
?confint
exp(confint(mod)) # Remember: transform to odds ratio using exp()
# An option for making a data.frame of confidence intervals and coefficients
confMod <- data.frame(cbind(lower = exp(confint(mod)[,1]),
coefs = exp(coef(mod)),
upper = exp(confint(mod)[,2])))
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
# Looking at this plot, which terms are significant at the 0.05 level?
## b) Factor vs numeric
#  The nsibs variable was parsed as an integer. The model.matrix() function is
#  used under the hood by lm() and glm() to create a design matrix of the model.
#  See the difference compared to when we input nsibs as an integer and a
#  factor:
?model.matrix
model.matrix( ~ unique(nsibs), data = graduation) # I see a problem with the data here...
# As a side note, we can use unique() with model.matrix() to create a matrix
# of different combinations of factor levels to use with predict(). Though it's
# probably not the best approach...
model.matrix( ~ as.factor(unique(nsibs)), data = graduation)
# A better function to help with this is expand.grid()
with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
# Consider for instance if we had a model just consisting of factors:
mod2 <- glm(hsgrad ~ nonwhite + mhs + fhs,
data = graduation,
family = "binomial")
predicted_data <- with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
# predicted_data是我们准备用于预测的新数据框，其中包含了所有可能的组合。
predicted_data <- cbind(predicted_data, predict(mod2,
newdata = predicted_data,
type = "response",
se = TRUE))
predicted_data <- cbind(predicted_data, predict(mod2,
newdata = predicted_data,
type = "response",
se = TRUE))
# Now we can use the code in Jeff's lecture to fill out the confidence intervals
# and predicted probability (see lecture)
# 这段代码使用within()函数向predicted_data数据框添加了三列：PredictedProb、LL和UL。
# PredictedProb列计算了逻辑回归模型的预测概率。它使用plogis()函数对模型的预测值（fit）进行逻辑转换，将其转换为0和1之间的概率值。
# LL列计算了预测概率的下限值，表示在95%的置信水平下的预测概率的下界。它使用模型的预测值减去1.96倍标准误差（se.fit），然后再进行逻辑转换。
# UL列计算了预测概率的上限值，表示在95%的置信水平下的预测概率的上界。它使用模型的预测值加上1.96倍标准误差（se.fit），然后再进行逻辑转换。
# 这样，predicted_data数据框中就包含了模型预测的概率值以及其对应的置信区间的上下界。
predicted_data <- within(predicted_data,
{PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
# 这样，predicted_data数据框中就包含了模型预测的概率值以及其对应的置信区间的上下界。
predicted_data <- within(predicted_data,
{PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
graduation$nsibs_cut <- cut(graduation$nsibs,
breaks = c(0, 0.9, 1, 3, Inf),
include.lowest = TRUE,
labels = c("None", "One", "Two_Three", "FourPlus"))
mod3 <- glm(hsgrad ~.,
data = graduation[,!names(graduation) %in% c("nsibs", "nsibs_f")],
family = "binomial")
summary(mod3)
summary(mod)
# Extract confidence intervals around the estimates
confMod3 <- data.frame(cbind(lower = exp(confint(mod3)[,1]),
coefs = exp(coef(mod3)),
upper = exp(confint(mod3)[,2])))
# Plot the estimates and confidence intervals
ggplot(data = confMod3, mapping = aes(x = row.names(confMod3), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
scale_y_continuous(breaks = seq(0,8,1)) +
labs(x = "Terms", y = "Coefficients")
